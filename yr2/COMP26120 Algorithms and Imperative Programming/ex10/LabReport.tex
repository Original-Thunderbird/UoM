\documentclass{article}

\title{COMP26120 Lab 13: Background}
\author{?}

\begin{document}
\maketitle

% PART 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The small-world hypothesis}
\label{sec:small world}
% Here give your statement of the small-world hypothesis and how you
% are going to test it.
Statement: If we represent each person as node and acquaintance of two as edge with length of 1, then for any two nodes there will be a path in between with its length no longer than 6.
Test: For every node in graph, set it as starting point and traverse the whole graph, see if there are often nodes that has to be reached by 7 steps or more from starting point, or even unreachable.

\section{Complexity Arguments}
\label{sec:complexity}
% Write down the complexity of Dijkstra's algorithm and of Floyd's algorithm.
% Explain why, for these graphs, Dijkstra's algorithm is more efficient.
Time complexity of Dijkstra's algorithm is O(n*(n+e)*log(n)); that of Floyd's algorithm is O(N^3).
For Oklahoma.gx, there are 17425 nodes and 1785057 edges, 442,489M time units is expected for Dijkstra's algorithm, 5,290,763M time units is expected for Floyd's algorithm.
For Caltech.gx, there are 769 nodes and 33313 edges, 251,262K time units is expected for Dijkstra's algorithm, 454,756K time units is expected for Floyd's algorithm.
We can see that for both two graphs Dijkstra's algorithm is expected to be more effcient.

\section{Part 1 results}
\label{sec:part1}
% Give the results of part one experiments.
For each starting node, I listed out nodes that have a distance larger than 6 with it and those unreachable and counted number of them respectively. I calculated average distance of reachable nodes and global average distance. The whole graph is very likely to meet the small-world property as no distance of two nodes is larger than 6(given that they are mutually erachable), yet we can see that 3 subgraphs are not connected to the largest one, thus for every node there exist nodes that are unreachable, showing that small-world property is not met.
Running time on Caltech.gx of all-pairs Dijkstra's algorithm is about 0.341s, on Oklahoma.gx is 2310.931s, 6776 times higher than that of Caltech.gx. 
If we put relation of nodes and edges of Caltech.gx and Oklahoma.gx to compute time complexity relationship, we get log(17425)/log(769)*17425/769*(17425+1785057)/(769+33313)=1761.135, not that far from actual result, thus execution times agree with the complexity characteristics found in Part 1.


% PART 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Part 2 complexity analysis}
\label{sec:complexity2}
% Give the complexity of the heuristic route finder.
My implementaton of heuristic route finder is basically based on BFS. In the worst case(or say ideally), all edges and nodes will be visited exactly once, so time complexity of single-source heuristic algorithm is O(n+e), for all-pairs it's O(n*(n+e)).

\section{Part 2 results}
\label{sec:part2}
% Give the results of part two experiments.
For heuristic search, running time is much faster than Dijkstra's algorithm, that's 0.057s for Caltech.gx and 208.20s for Oklahoma.gx. but its results get much less accurate, as for both graph we got much more distances that are larger than 6 and unreachable that don't appear under Dijkstra's searching. Thus there's a trade-off between accuracy and effciency: If running time is not a issue and enough space is given, then Dijkstra's algorithm is the best to get most accurate result; if effciency matters most and fallacies can be tolerated, then heuristic serach(probably a refined version, i.e. do search based on several nodes with larger out degree instead of just with the largest out degree)shall be a good choice.

\end{document}
